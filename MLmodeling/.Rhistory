#' If \code{covnames} includes an intercept, \code{d} needs to have column of 1s for the intercept
#' @param covnames Vector with the names of the intercept and covariates to be included in the formula
#' @return Vector with the cross-validation results
INLA_crossvali =  function(n, d, covnames){
print(n)
# Split data
smp_size <- floor(0.8 * nrow(d))
set.seed(n)
training <- sample(seq_len(nrow(d)), size = smp_size)
test <- seq_len(nrow(d))[-training]
# Fit model
# Data for prediction
dp <- d
dtraining <- d[training, ]
dptest <- dp[test, ]
# Fit model
lres <- fnFitModelINLA(dtraining, dptest, covnames, TFPOSTERIORSAMPLES = FALSE, formulanew = NULL)
# Get predictions
dptest <- fnGetPredictions(lres[[1]], lres[[2]], lres[[3]], dtraining, dptest, covnames, NUMPOSTSAMPLES = 0, cutoff_exceedanceprob = 30)
# Goodness of fit
val <- APMtools::error_matrix(validation = dptest$real, prediction = dptest$pred_mean)
val <- c(val, cor = cor(dptest$real, dptest$pred_mean))
(val <- c(val, covprob = mean(dptest$pred_ll <= dptest$real &  dptest$real <= dptest$pred_ul))) # 95% coverage probabilities
return(val)
}
##################################################
d <- read.csv("~/Documents/GitHub/uncertainty/data_vis_exp/DENL17_uc.csv")
# Covariates
covnames0 <- c("nightlight_450", "population_1000", "population_3000",
"road_class_1_5000", "road_class_2_100", "road_class_3_300", "trop_mean_filt",
"road_class_3_3000", "road_class_1_100", "road_class_3_100",
)
#"road_class_3_5000", "road_class_1_300", "road_class_1_500",
#"road_class_2_1000", "nightlight_3150", "road_class_2_300", "road_class_3_1000", "temperature_2m_7"
d <- d[, c("mean_value", "Longitude", "Latitude", covnames0)]
# covnames0 <- NULL
covnames <- c("b0", covnames0)  # covnames is intercept and covnames0
# Data for estimation. Create variables y with the response, coox and cooy with the coordinates, and b0 with the intercept (vector of 1s)
d$y <- d$mean_value # response
d$coox <- d$Longitude
d$cooy <- d$Latitude
d$b0 <- 1 # intercept
d$real <- d$y
# Data for prediction
dp <- d
# Call inla()
lres <- fnFitModelINLA(d, dp = NULL, covnames, TFPOSTERIORSAMPLES = FALSE, formulanew = NULL)
res <- lres[[1]]
stk.full <- lres[[2]]
mesh <- lres[[3]]
# Get predictions
dres <- fnGetPredictions(res, stk.full, mesh, d, dp, covnames, NUMPOSTSAMPLES = -1, cutoff_exceedanceprob = 30)
# Goodness of fit
APMtools::error_matrix(validation = dres$real, prediction = dres$pred_mean)
cor(dres$real, dres$pred_mean)
mean(dres$pred_ll <= dres$real &  dres$real <= dres$pred_ul)
# Cross-validation
VLA <- lapply(1:20, FUN = INLA_crossvali, d = d, covnames = covnames)
(VLA <- data.frame(LA = rowMeans(data.frame(VLA))))
# RMSE          7.5002554
# RRMSE         0.3136362
# IQR           7.4369859
# rIQR          0.3392630
# MAE           5.5197591
# rMAE          0.2308669
# rsq           0.6593912
# explained_var 0.6602920
# cor           0.8173491
# covprob       0.5226804
#install.packages("INLA", repos=c(getOption("repos"), INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
library(INLA)
#devtools::install_github("mengluchu/APMtools")
library(APMtools)
#' Creates triangulated mesh to fit a spatial model using INLA and SPDE
#'
#' @param coo coordinates to create the mesh
#' @return mesh boject
fnConstructMesh <- function(coo){
# meshbuilder()
# offset: size of the inner and outer extensions around the data locations
(offset1 <- 1/10*max(dist(coo)))
(offset2 <- 1/2*max(dist(coo)))
# max.edge: maximum allowed triangle edge lengths in the region and in the extension
(maxedge1 <- 1/20*max(dist(coo)))
(maxedge2 <- 1/3*max(dist(coo)))
# cutoff: minimum allowed distance between points used to avoid building many small triangles around clustered locations
(cutoff <- 1/10000*max(dist(coo)))
mesh <- inla.mesh.2d(loc = coo, offset = c(offset1, offset2), cutoff = cutoff, max.edge = c(maxedge1, maxedge2))
plot(mesh)
points(coo, col = "red")
print(mesh$n)
return(mesh)
}
#' Fits a spatial model using INLA and SPDE
#'
#' It creates a mesh using coordinates d$coox and d$cooy
#' Formula is intercept + covnames + spatial effect unless \code{formulanew} is specified
#' It creates stk.full with data for estimation or data for estimation and prediction (if TFPOSTERIORSAMPLES is TRUE)
#' Calls \code{inla()} and returns list with result and stk.full
#'
#' @param d Data frame with data for estimation that contains coordinates (coox, cooy), response variable (y) and covariates
#' If \code{covnames} includes an intercept, \code{d} needs to have column of 1s for the intercept
#' @param dp Data frame with data for prediction that contains coordinates (coox, cooy), and covariates
#' If \code{covnames} includes an intercept, \code{dp} needs to have column of 1s for the intercept
#' If dp is NULL, dp will not used to construct stk.full
#' @param covnames Vector with the names of the intercept and covariates to be included in the formula
#' @param TFPOSTERIORSAMPLES Boolean variable to call \code{inla()} with config = TFPOSTERIORSAMPLES.
#' If it config = TRUE we will get a res object with which we could call \code{inla.posterior.samples()}
#' @param formulanew A string with the formula. If formulanew is NULL, the formula will be constructed as intercept + covnames + spatial effect
#' @return list with the results of the fitted model, stk.full and mesh
fnFitModelINLA <- function(d, dp, covnames, TFPOSTERIORSAMPLES, formulanew = NULL){
# Coordinates locations
coo <- cbind(d$coox, d$cooy)
# Mesh
mesh <- fnConstructMesh(coo)
# Building the SPDE model on the mesh
spde <- inla.spde2.matern(mesh = mesh, alpha = 2, constr = TRUE)
# Index set
indexs <- inla.spde.make.index("s", spde$n.spde)
# Projection matrix
A <- inla.spde.make.A(mesh = mesh, loc = coo)
# Stack with data for estimation. Effects include intercept and covariates
stk.e <- inla.stack(tag = "est", data = list(y = d$y), A = list(1, A), effects = list(d[, covnames, drop = FALSE], s = indexs))
if(is.null(dp)){
stk.full <- inla.stack(stk.e)
}else{
# Prediction coordinate locations and projection matrix
coop <- cbind(dp$coox, dp$cooy)
Ap <- inla.spde.make.A(mesh = mesh, loc = coop)
# stack
stk.p <- inla.stack(tag = "pred", data = list(y = NA), A = list(1, Ap), effects = list(dp[, covnames, drop = FALSE], s = indexs))
stk.full <- inla.stack(stk.e, stk.p)
}
# Formula
# covnames includes the intercept and the covariates
if(is.null(formulanew)){
formula <- as.formula(paste0('y ~ 0 + ', paste0(covnames, collapse = '+'), " + f(s, model = spde)"))
}
else{
formula <- formulanew
}
# Call inla(). Add config = TRUE if we want to call inla.posterior.sample()
st1 <- Sys.time()
res <- inla(formula, data = inla.stack.data(stk.full), control.predictor = list(compute = TRUE, A = inla.stack.A(stk.full)),
control.compute = list(config = TFPOSTERIORSAMPLES))
st2 <- Sys.time()
print(st2-st1)
return(list(res, stk.full, mesh))
}
#' Computes the linear predictor from one of the samples of an object obtained with \code{inla.posterior.samples()
#'
#' It retrieves the sample number \code{ite} from the object \code{psamples} that was obtained with \code{inla.posterior.samples()
#' For this sample, it extracts the betas for the coefficients in \code{covnames} and the values of the spatial field
#' Then it calculates beta*covariates + spatial effect
#'
#' @param psamples Object obtained from \code{inla.posterior.samples() that contains a list with the samples
#' @param ite Number of sample from \code{psamples} that we want to use
#' @param res result object from an \code{inla()} call
#' @param mesh Triangulated mesh that was used to fit the model
#' @param dp Data frame with data for prediction that contains coordinates (coox, cooy), and covariates.
#' If \code{covnames} includes an intercept, \code{dp} needs to have column of 1s for the intercept
#' @param covnames Name of the coefficients in the formula (intercept and other covariates)
#' @return Data frame \code{dp} with added columns \code{pred_mean}, \code{pred_ll}, \code{pred_ul} and \code{excprob}
fnPredictFromPosteriorSample <- function(psamples, ite, res, mesh, dp, covnames){
# Retrieve elements sample
(contents <- res$misc$configs$contents)
# betas for elements of covnames. covnames[1] is the first covariate (b0)
id_firstcov <- grep(covnames[1], rownames(psamples[[ite]]$latent))
betas <- psamples[[ite]]$latent[id_firstcov : (id_firstcov + (length(covnames)-1)), ]
# spatial field
id_s <- which(contents$tag == "s")
id_s <- contents$start[id_s]:(contents$start[id_s] + contents$length[id_s] - 1)
spatialfield <- psamples[[ite]]$latent[id_s]
# spat <- lapply(ps, function(x) x$latent[id_s])
# spat <- matrix(unlist(spat), ncol = length(id_s), byrow = T)
# Multiply model matrix times betas + spatial effect
coop <- cbind(dp$coox, dp$cooy)
Ap <- inla.spde.make.A(mesh = mesh, loc = coop)
predictions <- as.matrix(dp[, covnames]) %*% betas + drop(Ap %*% spatialfield)
return(predictions)
}
#' Get predictions from a result object obtained by fitting as spatial model using INLA and SPDE
#'
#' @param res result object from an \code{inla()} call
#' @param stk.full stk.full object constructed during an \code{inla()} call
#' @param mesh Triangulated mesh that was used to fit the model
#' @param covnames Name of the coefficients in the formula (intercept and other covariates)
#' @param d Data frame with data for estimation that contains coordinates (coox, cooy), response variable (y) and covariates
#' If \code{covnames} includes an intercept, \code{d} needs to have column of 1s for the intercept
#' It can be NULL if predictions are added to \code{dp}
#' @param dp Data frame with data for prediction that contains coordinates (coox, cooy), and covariates
#' If \code{covnames} includes an intercept, \code{dp} needs to have column of 1s for the intercept
#' If dp is NULL, dp will not used to construct stk.full
#' It can be NULL if predictions are added to \code{d}
#' @param NUMPOSTSAMPLES number of samples to call \code{inla.posterior.samples()}
#' If NUMPOSTSAMPLES == -1, get the predictions directly from the "est" elements of res and add them to \code{d}
#' If NUMPOSTSAMPLES == 0, get the predictions directly from the "pred" elements of res and add them to \code{dp}
#' If NUMPOSTSAMPLES > 0, get the predictions  using \code{inla.posterior.samples()} and add them to \code{dp}.
#' If NUMPOSTSAMPLES > 0, \code{dp} may or may not have passed previously to \code{inla()}
#' @param cutoff_exceedanceprob cutoff value to compute exceedance probabilities P(theta > cutoff)
#' @return Data frame \code{d} or \code{dp} with added columns \code{pred_mean}, \code{pred_ll}, \code{pred_ul} and \code{excprob}
#' \code{pred_mean} is the posterior mean
#' \code{pred_ll} and \code{pred_ul} are the lower and upper limits of 95% credible intervals
#' \code{excprob} is the probability that hte prediction > cutoff value
fnGetPredictions <- function(res, stk.full, mesh, d, dp, covnames, NUMPOSTSAMPLES, cutoff_exceedanceprob){
if(NUMPOSTSAMPLES == -1){
index <- inla.stack.index(stk.full, tag = "est")$data
d$pred_mean <- res$summary.fitted.values[index, "mean"]
d$pred_ll <- res$summary.fitted.values[index, "0.025quant"]
d$pred_ul <- res$summary.fitted.values[index, "0.975quant"]
d$excprob <- sapply(res$marginals.fitted.values[index],
FUN = function(marg){1-inla.pmarginal(q = cutoff_exceedanceprob, marginal = marg)})
dres <- d
}
if(NUMPOSTSAMPLES == 0){
index <- inla.stack.index(stk.full, tag = "pred")$data
dp$pred_mean <- res$summary.fitted.values[index, "mean"]
dp$pred_ll <- res$summary.fitted.values[index, "0.025quant"]
dp$pred_ul <- res$summary.fitted.values[index, "0.975quant"]
dp$excprob <- sapply(res$marginals.fitted.values[index],
FUN = function(marg){1-inla.pmarginal(q = cutoff_exceedanceprob, marginal = marg)})
dres <- dp
}
if(NUMPOSTSAMPLES > 0){
psamples <- inla.posterior.sample(NUMPOSTSAMPLES, res)
ps <- sapply(1:NUMPOSTSAMPLES, fnPredictFromPosteriorSample, psamples = psamples, res = res, mesh = mesh, dp = dp, covnames = covnames)
dp$pred_mean <- rowMeans(ps)
dp$pred_ll <- apply(ps, 1, function(x){quantile(x, 0.025)})
dp$pred_ul <- apply(ps, 1, function(x){quantile(x, 0.975)})
dp$excprob <- apply(ps, 1, function(x){mean(x > cutoff_exceedanceprob)})
dres <- dp
}
return(dres)
}
#' Calculates cross-validation measures obtained by fitting a spatial model using INLA and SPDE
#'
#' @param n Number of iteration
#' @param d Data frame with data for estimation that contains coordinates (coox, cooy), response variable (y) and covariates
#' If \code{covnames} includes an intercept, \code{d} needs to have column of 1s for the intercept
#' @param covnames Vector with the names of the intercept and covariates to be included in the formula
#' @return Vector with the cross-validation results
INLA_crossvali =  function(n, d, covnames){
print(n)
# Split data
smp_size <- floor(0.8 * nrow(d))
set.seed(n)
training <- sample(seq_len(nrow(d)), size = smp_size)
test <- seq_len(nrow(d))[-training]
# Fit model
# Data for prediction
dp <- d
dtraining <- d[training, ]
dptest <- dp[test, ]
# Fit model
lres <- fnFitModelINLA(dtraining, dptest, covnames, TFPOSTERIORSAMPLES = FALSE, formulanew = NULL)
# Get predictions
dptest <- fnGetPredictions(lres[[1]], lres[[2]], lres[[3]], dtraining, dptest, covnames, NUMPOSTSAMPLES = 0, cutoff_exceedanceprob = 30)
# Goodness of fit
val <- APMtools::error_matrix(validation = dptest$real, prediction = dptest$pred_mean)
val <- c(val, cor = cor(dptest$real, dptest$pred_mean))
(val <- c(val, covprob = mean(dptest$pred_ll <= dptest$real &  dptest$real <= dptest$pred_ul))) # 95% coverage probabilities
return(val)
}
##################################################
d <- read.csv("~/Documents/GitHub/uncertainty/data_vis_exp/DENL17_uc.csv")
# Covariates
covnames0 <- c("nightlight_450", "population_1000", "population_3000",
"road_class_1_5000", "road_class_2_100", "road_class_3_300", "trop_mean_filt",
"road_class_3_3000", "road_class_1_100", "road_class_3_100"
)
#"road_class_3_5000", "road_class_1_300", "road_class_1_500",
#"road_class_2_1000", "nightlight_3150", "road_class_2_300", "road_class_3_1000", "temperature_2m_7"
d <- d[, c("mean_value", "Longitude", "Latitude", covnames0)]
# covnames0 <- NULL
covnames <- c("b0", covnames0)  # covnames is intercept and covnames0
# Data for estimation. Create variables y with the response, coox and cooy with the coordinates, and b0 with the intercept (vector of 1s)
d$y <- d$mean_value # response
d$coox <- d$Longitude
d$cooy <- d$Latitude
d$b0 <- 1 # intercept
d$real <- d$y
# Data for prediction
dp <- d
# Call inla()
lres <- fnFitModelINLA(d, dp = NULL, covnames, TFPOSTERIORSAMPLES = FALSE, formulanew = NULL)
res <- lres[[1]]
stk.full <- lres[[2]]
mesh <- lres[[3]]
# Get predictions
dres <- fnGetPredictions(res, stk.full, mesh, d, dp, covnames, NUMPOSTSAMPLES = -1, cutoff_exceedanceprob = 30)
# Goodness of fit
APMtools::error_matrix(validation = dres$real, prediction = dres$pred_mean)
cor(dres$real, dres$pred_mean)
mean(dres$pred_ll <= dres$real &  dres$real <= dres$pred_ul)
# Cross-validation
VLA <- lapply(1:20, FUN = INLA_crossvali, d = d, covnames = covnames)
(VLA <- data.frame(LA = rowMeans(data.frame(VLA))))
# RMSE          7.5002554
# RRMSE         0.3136362
# IQR           7.4369859
# rIQR          0.3392630
# MAE           5.5197591
# rMAE          0.2308669
# rsq           0.6593912
# explained_var 0.6602920
# cor           0.8173491
# covprob       0.5226804
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path=paste0('global_crossvali',"/"),
echo=F, warning=FALSE, message=FALSE, dev = "png", include = T)
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE , repos='http://cran.muenster.r-project.org')
sapply(pkg, require, character.only = TRUE)
}
packages <- c( "devtools", "dplyr","data.table" , "ggplot2" , "RColorBrewer", "xgboost",  "glmnet", "ranger", "randomForest","tidyr" ,"tibble","stargazer", "sf", "CAST", "caret", "quantregForest", "pdp", "h2o")
ipak(packages)
install_github("mengluchu/APMtools")
library(APMtools)
resolution =100 # resolution of grid
nboot = 2  # number of bootstraps
y_var = "mean_value"
prestring =  "road|nightlight|population|temp|wind|trop|indu|elev|radi"
varstring = paste(prestring,y_var,sep="|")
mergedall = read.csv("~/Documents/GitHub/uncertainty/data_vis_exp/DENL17_uc.csv")
merged = mergedall%>%dplyr::select(matches(varstring))%>% na.omit() # there is actually no na in this file, but for now RF and LA doesnt deal with missing data, leave out for quick examination
if (resolution ==100)
{
merged = merged%>%dplyr::select(-c(industry_25,industry_50,road_class_1_25,road_class_1_50,road_class_2_25,road_class_2_50,   road_class_3_25,road_class_3_50))
}
names(merged)
n =1
df =merged
set.seed(n)
smp_size <- floor(0.8 * nrow(df))
training<- sample(seq_len(nrow(df)), size = smp_size)
test = seq_len(nrow(df))[-training]
y_denl = df[,y_var]
y_denl_test = y_denl[test]
x_p = df%>%dplyr::select(-y_var)
s.seq <- sort(c( seq(10, 40, by = 3),
seq(41, ncol(x_p), by = 5) ), decreasing = T)
# collect results in list
qrf.elim <- oob.mse <-cv<- list()
# save model and OOB error of current fit
ini = ranger(x=x_p, y=y_denl, num.threads = 10, # set the number of CPUs
,importance = "permutation")
qrf.elim[[1]] <- ini
oob.mse[[1]] <- qrf.elim[[1]]$prediction.error
cv[[1]] <- sqrt(qrf.elim[[1]]$prediction.error)+1
l.covar.sel <- ncol(x_p)
# Iterate through number of retained covariates
for( ii in 1:length(s.seq) ){
# Get importance, decreasingly ordered
t.imp <- qrf.elim[[ii]]$variable.importance[
order(qrf.elim[[ii]]$variable.importance, decreasing = T) ]
qrf.elim[[ii+1]] <- ranger(y = y_denl, x= x_p[, names(t.imp[1:s.seq[ii]])], num.trees = 1000,
num.threads = 20, # set the number of CPUs
importance = "permutation")
oob.mse[[ii+1]] <- qrf.elim[[ii+1]]$prediction.error
out = rf_LUR(df[, c(names(t.imp[1:s.seq[ii]]), y_var)], numtrees =  1000, mtry = NULL, vis1 = F,y_varname= y_var, training=training, test=test, grepstring =varstring)
cv[[ii+1]] = out[1]
}
# Prepare a data frame for plot
elim.oob <- data.frame(elim.n = c(ncol(x_p), s.seq[1:length(s.seq)]),
elim.OOBe = unlist(oob.mse), cv = unlist(cv) )
## ----plot-selection-path,fig.align='center',echo=FALSE,fig.height = 5,out.width='0.8\\textwidth',fig.cap = "Path of out-of-bag mean squared error as covariates are removed. Minimum is found at 55 covariates."----
plot(elim.oob$elim.n[2:(length(elim.oob$elim.n) )], elim.oob$cv[2:(length(elim.oob$elim.n))],
ylab = "cv (rmse)",
xlab = "n covariates",
pch = 20)
abline(v = elim.oob$cv[ which.min(elim.oob$cv)], lty = "dotted")
## ----quantRF,cache=TRUE-------------------------------------------
# Fit quantile regression forest
smp_size <- floor(0.8 * nrow(df))
training<- sample(seq_len(nrow(df)), size = smp_size)
test = seq_len(nrow(df))[-training]
y_denl = df[,y_var]
y_denl_test = y_denl[test]
x_p = df%>%dplyr::select(-y_var)
quantRF <- ranger(x = x_p[training,],
y = y_denl[training], mtry = NULL, num.trees = 1000,
quantreg = T)
# compute predictions (mean) for each validation site
pred <- predict(quantRF, data = x_p[test,], what = mean)
## ----investigate-single-point,echo=FALSE,fig.pos='!h',fig.height=5,fig.width=4,fig.align='center', out.width='0.4\\textwidth',fig.cap= "Histogram of predictive distribution for one single prediction point (dotted lines: 90 \\% prediction interval, dashed line: mean prediction)."----
## predict 0.01, 0.02,..., 0.99 quantiles for validation data
pred.distribution <- predict(quantRF,
data = x_p[test,],
type = "quantiles",
quantiles = seq(0.01, 0.99, by = 0.01))
# plot predictive distribution for one site
hist( pred.distribution$predictions[1,],
col = "grey", main = "",
xlab = "predicted NO2", breaks = 12)
# add 90 % prediction interval and mean (dashed) to plot
abline(v = c( pred.distribution$predictions[1, "quantile= 0.05"],
pred.distribution$predictions[1, "quantile= 0.95"]),
lty = "dotted")
abline(v = pred$predictions[1], lty = "dashed")
## ----create-intervall-plot,fig.height=5,fig.align='center',echo=FALSE, out.width='0.8\\textwidth',fig.cap= "Coverage of 90 \\%-prediction intervals computed by model-based boostrap."----
# get 90% quantiles for each point
t.quant90 <- cbind(
pred.distribution$predictions[, "quantile= 0.05"],
pred.distribution$predictions[, "quantile= 0.95"])
# get index for ranking in the plot
t.ix <- sort( pred$predictions, index.return = T )$ix
# plot predictions in increasing order
plot(
pred$predictions[t.ix], type = "n",
ylim = range(c(t.quant90,  pred$predictions, y_denl[test])),
xlab = "rank of predictions",  # Me: predictors?
ylab =  "NO2"
)
# add prediction intervals
segments(
1:length( y_denl_test ),
t.lower <- (t.quant90[,1])[t.ix],
1:length(y_denl_test ),
t.upper <- (t.quant90[,2])[t.ix],
col = "grey"
)
# select colour for dots outside of intervals
t.col <- sapply(
1:length( t.ix ),
function( i, x, lower, upper ){
as.integer( cut( x[i], c( -Inf, lower[i]-0.000001,
x[i], upper[i]+0.000001, Inf ) ) )
},
x = y_denl_test[t.ix],
lower = t.lower, upper = t.upper
)
# add observed values on top
points(
1:length( y_denl_test) ,
y_denl_test[t.ix], cex = 0.7,
pch = c( 16, 1, 16)[t.col],
col = c( "darkgreen", "black", "darkgreen" )[t.col]
)
points(pred$predictions[t.ix], pch = 16, cex = 0.6, col = "grey60")
# Add meaningfull legend
legend( "topleft",
bty = "n", cex = 0.85,
pch = c( NA, 16, 1, 16 ), pt.cex = 0.6, lwd = 1,
lty = c( 1, NA, NA, NA ),
col = c( "grey", "grey60", "black", "darkgreen" ),
seg.len = 0.8,
legend = c(
"90 %-prediction interval",
paste0("prediction (n = ", nrow(x_p[test,]), ")"),
paste0("observation within interval (n = ",
sum( t.col %in% c(2) ), ")" ),
paste0("observation outside interval (n = ",
sum( t.col %in% c(1,3)), ", ",
round(sum(t.col %in% c(1,3)) /
nrow(x_p[training,])*100,1), "%)") )
)
## ----create-coverage-probabilty-plots,fig.align='center', fig.pos = "h", fig.width=4,fig.height=4.5, out.width='0.45\\textwidth',fig.cap="Coverage probabilities of one-sided prediction intervals"----
# Coverage probabilities plot
# create sequence of nominal probabilities
ss <- seq(0,1,0.01)
# compute coverage for sequence
t.prop.inside <- sapply(ss, function(ii){
boot.quantile <-  t( apply(pred.distribution$predictions, 1, quantile,
probs = c(0,ii) ) )[,2]
return( sum(boot.quantile <= y_denl_test)/nrow(x_p[test,]) )
})
plot(x = ss, y = t.prop.inside[length(ss):1],
type = "l", asp = 1,
ylab = "coverage probabilities",
xlab="nominal probabilities" )
# add 1:1-line
abline(0,1, lty = 2, col = "grey60")
# add lines of the two-sided 90 %-prediction interval
abline(v = c(0.05, 0.95), lty = "dotted", col = "grey20")
## ----session-info,results='asis'----------------------------------
sessionInfo()
## ----export-r-code,echo=FALSE,result="hide"-----------------------
# purl("OpenGeoHub-machine-learning-training-2.Rnw")
t.prop.inside
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path=paste0('global_crossvali',"/"),
echo=F, warning=FALSE, message=FALSE, dev = "png", include = T)
ipak <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
if (length(new.pkg))
install.packages(new.pkg, dependencies = TRUE , repos='http://cran.muenster.r-project.org')
sapply(pkg, require, character.only = TRUE)
}
packages <- c( "devtools", "dplyr","data.table" , "ggplot2" , "RColorBrewer", "xgboost",  "glmnet", "ranger", "randomForest","tidyr" ,"tibble","stargazer", "sf", "CAST", "caret", "quantregForest", "pdp", "h2o")
ipak(packages)
install_github("mengluchu/APMtools")
library(APMtools)
ls("package:APMtools")
resolution =100 # resolution of grid
nboot = 2  # number of bootstraps
y_var = "mean_value"
prestring =  "road|nightlight|population|temp|wind|trop|indu|elev|radi"
varstring = paste(prestring,y_var,sep="|")
mergedall = read.csv("https://raw.githubusercontent.com/mengluchu/uncertainty/master/data_vis_exp/DENL17_uc.csv")
merged = mergedall%>%dplyr::select(matches(varstring))%>% na.omit() # there is actually no na in this file, but for now RF and LA doesnt deal with missing data, leave out for quick examination
if (resolution ==100)
{
merged = merged%>%dplyr::select(-c(industry_25,industry_50,road_class_1_25,road_class_1_50,road_class_2_25,road_class_2_50,   road_class_3_25,road_class_3_50))
}
names(merged)
q_rf_crossvali =  function(n,df, y_var) {
smp_size <- floor(0.8 * nrow(df))
set.seed(n)
training<- sample(seq_len(nrow(df)), size = smp_size)
test = seq_len(nrow(df))[-training]
q_rf_LUR(df, numtrees =  1000, mtry = NULL,  y_varname= y_var, training=training, test=test, grepstring =varstring)}
qrf = lapply(1:nboot, df = merged, y_var = y_var, q_rf_crossvali )
